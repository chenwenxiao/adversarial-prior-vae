\section{Training and Sampling}\label{sec:vaepp}
In this section, we will propose two training methods and a sampling method for VAEPP. The main difference between these two trainings method is how to train the discriminator. 

\subsection{2-step training for VAEPP} \label{subsec:naive_vaepp}
The discriminator should be obtained by $W^1(p_\theta, p^*)$, suggested by  WGAN~\cite{arjovsky2017wasserstein}. However in VAEPP, $p_\theta$ is intractable for sampling, since $p_\theta(x) = \E_{p_\lambda(z)} p_\theta(x|z)$ and $p_\lambda(z)$ is intractable for sampling. 

When $\beta$ is small enough, $p_\lambda(z)$ is near to $p_\mathcal{N}(z)$ which is feasible for sampling. Then, $p_\theta(x)$ is near to $p^\dag(x)$, where $p^\dag(x) = \E_{p_\mathcal{N}(z)} p_\theta(x|z)$ and $p^\dag(x)$ is feasible for sampling. Therefore, we suspect that $W^1(p_\theta, p^*)$ could be replace by $W^1(p^\dag, p^*)$, which is feasible since $p^\dag(x)$ and $p^*(x)$ are feasible for sampling. $\beta$ is limited by a hyper-parameter to ensure small $\beta$. 
In this way, an discriminator $D$ is trained by:
\begin{equation*}
	W^1(p^\dag, p^*) = \sup_{Lip(D) \leq 1} \E_{p^\dag(x)} D(x) - \E_{p^*(x)} D(x)
\end{equation*} 

\begin{figure}[tb]
	\centering
	\includegraphics[width=1.0\columnwidth]{../dist.strip/loss_curves}
	\caption{
	Training loss of Naive VAEPP and VAEPP on CIFAR-10. Naive VAEPP is more unstable and nearly crashes at 80 epoch while VAEPP has a little acceptable gap. From global view, the training loss of VAEPP is more smooth than Naive VAEPP and is better than Naive VAEPP's over almost all training process, which validates the motivation in \cref{subsec:improve_of_vaepp}. There are little gaps at per 200 epoch because learning rate is reduced to half at every 200 epoch. 
	}
	\label{fig:loss_curves}
\end{figure}
%When $\beta$ is too large, the discriminator could not assess the quality of data as we expect, since $W^1(p_\theta, p^*)$ might be greatly different to $W^1(p^\dag, p^*)$. 
%Fortunately, the training for $\beta$ can avoid that $\beta$ becomes too large. 
%When $\beta$ becomes too large, the discriminator could not assess the quality of data as we expect. Therefore, ELBO will be worse and then $\beta$ will decrease. 
The other parameters of VAEPP are trained by SGVB:
\begin{equation*}
	\max_{\theta, \phi, \beta} \mathcal{L}(\theta, \phi, \beta, \omega)
\end{equation*}
Above two optimizations run alternatively, which is called the 2-step training algorithm for VAEPP, shown in \cref{alg:vaepp}. The model trained by 2-step training algorithm is called Naive VAEPP. 
\begin{algorithm}[tb]
\caption{2-step training algorithm for VAEPP}
\label{alg:vaepp}
\textbf{Require}: The gradient penalty algorithm $R$, the batch size $b$, the number of critic iterations per generator iteration $n_c$, the parameters for Adam Optimizers, $\tau$. 

\begin{algorithmic}[1] %[1] enables line numbers
\WHILE{$\theta, \phi, \beta, \omega$ have not converged}
\FOR {$k = 1, \ldots n_c$}
\FOR {$i = 1, \ldots, b$}
\STATE Sample $e, x \sim p^*$, $z \sim q_\phi(z|e)$, $\epsilon \sim p_\mathcal{N}$
\STATE $Z^{(i)} \gets \frac{1}{2}(e^{-\beta D(G(\epsilon))} + \frac{f_\lambda(z)}{\frac{1}{M} q_\phi(z|e)})$
\STATE $\mathcal{L}^{(i)} \gets \ln p_\theta(x|z) + \ln f_\lambda(z) - \ln q_\phi(z|e)$
\ENDFOR
\STATE $\mathcal{L} \gets \frac{1}{b}\sum_{i}^b \mathcal{L}^{(i)} - \ln (\frac{1}{b}\sum_{i}^b Z^{(i)})$
\STATE $\theta, \phi, \beta \gets $ Adam $(\nabla_{\theta, \phi, \beta} \mathcal{L}, \{\theta, \phi, \beta\}, \tau)$
\ENDFOR
\FOR {$i = 1, \ldots, b$}
\STATE Sample $e, x \sim p^*$, latent variable $z \sim p_\mathcal{N}$
\STATE	$\hat{e} = G(z)$, get gradient penalty term $\zeta \gets R(e, \hat{e})$
\STATE $L^{(i)} \gets D(\hat{x}) - D(x) + \zeta$
\ENDFOR
\STATE $\omega \gets $ Adam $(\nabla_{\omega} \frac{1}{b}\sum_{i}^b L^{(i)}, \omega, \tau)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{1-step training for VAEPP} \label{subsec:improve_of_vaepp}

However, the training process of \cref{alg:vaepp} is unstable and inefficient, as shown in \cref{fig:loss_curves}. 
We suspect that the two independent optimizations instead of one whole optimization, may lower the log-likelihood and stability. Therefore, we try to combine the training for $\theta, \phi, \beta, \omega$ into a whole optimization. 
Our solution is to use SGVB with the gradient penalty term to train VAEPP:
\begin{equation*}~\label{eq:1_step_optimization}
	\max_{\theta, \phi, \beta} \max_{Lip(D) \leq 1} \mathcal{L}(\theta, \phi, \beta, \omega) 
\end{equation*} 
The optimization $\max_{Lip(D) \leq 1} \mathcal{L}(\theta, \phi, \beta, \omega)$ is equivalent to optimization $\max_{Lip(D) \leq 1} \{ -\E_{q_\phi(z)} \beta*D(G(z)) - \ln Z \}$. 
The latter is a lower-bound of $\beta W^1(p^\dag, p^*)$:
\begin{align*}\label{eq:improve_vaepp}
	&\max_{Lip(D) \leq 1} \{ -\E_{q_\phi(z)} \beta*D(G(z)) - \ln Z \}\\ 
	&\leq \beta \max_{Lip(D) \leq 1} \{ \E_{p_\mathcal{N}(z)} D(G(z)) - E_{q_\phi(z)} D(G(z)) \} \\
	&= \beta \max_{Lip(D) \leq 1} \{ \E_{p^\dag(x)} D(x) - E_{p_r(x)} D(x) \} \\
	&= \beta W^1(p^\dag, p_r) \approx \beta W^1(p^\dag, p^*) \tag{10} 
\end{align*}
where $p_r(x) = \E_{q_\phi(z)} p_\theta(x|z)$ and the inequality of $\ln Z$ is:
\begin{equation*}
	\ln Z = \ln \E_{p_\mathcal{N}(z)} e^{- \beta * D(G(z))} \geq \E_{p_\mathcal{N}(z)} [- \beta * D(G(z))]
\end{equation*}
The last approximation sign in \cref{eq:improve_vaepp} is from our expectation that $p_r \rightarrow p^*$, which is also observed in experiments, after few epochs in training. \cref{eq:improve_vaepp} also uses the assumption $\E_{p_\theta(x|z)} D(x) = D(\E_{p_\theta(x|z)} x) = D(G(z))$ introduced in \cref{subsec:inference} to simplify equation at the 2nd column. 

\cref{eq:improve_vaepp} indicates that it is reasonable to obtain  discriminator $D$ during optimizing \cref{eq:1_step_optimization}. \cref{eq:improve_vaepp} also indicates that the gradient penalty term should be multiplied by $\beta$. Finally, the optimizations for $\theta, \phi, \beta$ and $\omega$ are combined into one, which is called the 1-step training algorithm for VAEPP, shown in \cref{alg:improved_vaepp}. The model trained by 1-step training algorithm is called VAEPP. 
%Moreover, the discriminator trained by \cref{alg:vaepp} and in \cref{alg:improved_vaepp}, are both approximations to the real discriminator in $W^1(p_\theta, p^*)$. 

%Thanks to the gradient penalty terms provided by WGAN-GP and WGAN-div, we enjoy stable and efficient training. The model trained by \cref{alg:vaepp} is called Naive VAEPP and the model trained by \cref{alg:improved_vaepp} is called VAEPP. 
\begin{algorithm}[tb]
\caption{1-step training algorithm for VAEPP}
\label{alg:improved_vaepp}
\textbf{Require}: The gradient penalty algorithm $R$, the batch size $b$, the parameters for Adam Optimizers, $\tau$. 

\begin{algorithmic}[1] %[1] enables line numbers
\WHILE{$\theta, \phi, \beta, \omega$ have not converged}
\FOR {$i = 1, \ldots, b$}
\STATE Sample $e, x \sim p^*$, $z \sim q_\phi(z|e)$, $\epsilon \sim p_\mathcal{N}$
\STATE $\hat{e} = G(\epsilon)$, get gradient penalty term $\zeta \gets R(e, \hat{e})$ 
\STATE $Z^{(i)} \gets \frac{1}{2}(e^{-\beta D(G(\epsilon))} + \frac{f_\lambda(z)}{\frac{1}{M} q_\phi(z|e)})$
\STATE $\mathcal{L}^{(i)} \gets \ln p_\theta(x|z) + \ln f_\lambda(z) - \ln q_\phi(z|e) + \beta \zeta$
\ENDFOR
\STATE $\mathcal{L} \gets \frac{1}{b}\sum_{i}^b \mathcal{L}^{(i)} - \ln (\frac{1}{b}\sum_{i}^b Z^{(i)})$
\STATE $\theta, \phi, \beta, \omega \gets $ Adam $(\nabla_{\theta, \phi, \beta} \mathcal{L}, \{\theta, \phi, \beta, \omega\}, \tau)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Sampling from VAEPP}
We apply Langevin dynamics to sample $z$ from $p_\lambda(z)$. It could generate natural and sharp images and only requires that $\nabla_z \log p_\lambda(z)$ is computable and $p_\lambda(z_0)$ is high enough where $z_0$ is the initial point of Langevin dynamics~\cite{song2019generative}. 
Moreover, \cite{kumar2019maximum} has implemented a Metropolis-Adjusted Langevin Algorithm (MALA) for sampling, where the formula of density also contains a discriminator term. 
But how to obtain the initial $z_0$ whose density is high enough is still a problem. 

Following the philosophy of VAEPP, \IE using the technique of GANs to assist VAEs, it is natural to use a GAN to model the distribution $q_\phi(z)$, and use samples of the GAN as the initial $z_0$ for MALA. 

The sampling of VAEPP consists of 3 parts: 
\begin{enumerate}
	\item generate initial $z_0$ by a GAN modeling $q_\phi(z)$
	\item generate $z \sim p_\lambda(z)$ from initial $z_0$ by MALA
	\item generate image from $z$ with the decoder
\end{enumerate}

This sampling process is similar to 2-Stage VAE~\cite{dai2019diagnosing}. The main difference between them is that VAEPP applies Langevin dynamics to sample from the explicit prior but 2-Stage VAE doesn't, since the prior of 2-Stage VAE is implicit. In experiments, we found that sampling from the explicit learnable prior might improve the quality of sampling in some datasets. 

%It is hard to sample $z$ from $p_\lambda(z)$ since it is complicated. 
Accept-Reject Sampling (ARS)~\cite{bauer2019resampled} is useless for $p_\lambda$ since ARS requires that $p_\lambda(z) / p_\mathcal{N}(z)$ is bounded by a constant $M$ on the support of $p_\lambda$, such that a sample could be accepted in expected $M$ times. But it is hard to ensure that there exists an enough small $M$ in VAEPP.
 
 