\section{Background}

\subsection{VAE and learnable prior}

Many generative models aim to minimize the KL-divergence between empirical distribution $p^*(x)$ and model distribution $p_\theta(x)$, which leads to maximization of log-likelihood. VAE~\cite{kingma2014auto} models the joint distribution $p_\theta(x, z)$ and the marginal distribution is $p_\theta(x) = \int p_\theta(x, z) \dd z$. VAE apply variational inference to obtain the evidence lower bound objective (ELBO): 
\begin{align*} \label{eq:ELBO}
\ln p_\theta(x) \geq \E_{q_\phi(z|x)} [  &  \ln p_\theta(x|z) + \ln p_\theta(z) - \\ & \ln q_\phi(z|x) ] 
\triangleq \mathcal{L}(x; \theta, \phi) \tag{1}
\end{align*}
where $q_\phi(z|x)$ is variational posterior (encoder) and $p_\theta(x|z)$ is true posterior (decoder). The training objective of VAE is $\EEE{p^*(x)}{\mathcal{L}(x; \theta, \phi)}$ and it is optimized by SGVB with re-parameterization trick. In vanilla VAE, prior $p_\theta(z)$ is chosen as the standard Gaussian distribution. 

Recently, some researchers show that the simplistic prior could lead to underfitting and many learnable priors are proposed subsequently to enrich prior~\cite{tomczak2018vae}. Most of them focus on the aggregated posterior $q_\phi(z)$, which is shown as the optimal prior for ELBO by following decomposition where $p_\lambda(z)$ denotes the prior maybe learnable:
\begin{align*} \label{eq:second-decomposition}
\mathcal{L}(\theta, \phi, \lambda) = \E_{p^*(x)}\E_{q_\phi(z|x)} [ \ln p_\theta(x|z)] + \\ 
\E_{p^*(x)}[\mathbb{H}[q_\phi(z|x)]] + \E_{q_\phi(z)} \ln p_\lambda(z) = \mathcal{I} + \mathcal{J} + \mathcal{K} \tag{2}
\end{align*}
where $\mathcal{I}, \mathcal{J}, \mathcal{K}$ denotes 3 terms respectively. 
Notice that $p_\lambda(z)$ only appears in the last term $\mathcal{K}$ and the optimal solution of $p_\lambda(z)$ is $q_\phi(z)$. However, $q_\phi(z)$ is intractable. \cite{tomczak2018vae,takahashi2019variational} tries to approximate it but reaches limited performance. 

\subsection{GAN and Wasserstein distance}

In vanilla GAN, a generator is trained to generate samples for deceiving discriminator, and a discriminator to distinguish the generated samples and real samples. However, vanilla GAN is unstable in training process. WGAN introduce Wasserstein distance to tackle this problem. 1st Wasserstein distance $W^1(\mu, \nu)$ is used for calculating the distance between two measures $\mu, \nu$. The dual form of Wasserstein distance is following:
\begin{align*} \label{eq:Wasserstein-distance}
W^1(\mu, \nu) = \sup_{Lip(D) \leq 1} \{\E_{\mu(x)} D(x)  - \E_{\nu(x)} D(x)\} \tag{3}
\end{align*}
where $Lip(D) \leq 1$ means $D$ is 1-Lipschitz. WGAN is optimized by minimizing $W^1(p^*, p_\theta)$ which can be seen as a min-max optimization, whose parameters are $D$ and $\theta$. 

WGAN makes progress toward stable training but sometimes fails to converge due to the use of weight clipping to enforce the Lipschitz constrain. WGAN-GP~\cite{gulrajani2017improved} and WGAN-div~\cite{wu2018wasserstein} improve WGAN by gradient penalty technique to achieve a more stable training. These techniques make WGAN framework become more robust and stable. 
