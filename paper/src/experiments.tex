\section{Experiments}
VAEPP is evaluated in common datasets including MNIST, Static-MNIST\cite{larochelle2011neural}, Fashion-MNIST~\cite{xiao2017/online}, Omniglot~\cite{lake2015human}, and CIFAR-10~\cite{krizhevsky2009learning} using log-likelihood. The quality of sampling of VAEPP is evaluated in MNIST, Fashion-MNIST, CIFAR-10 and CelebA~\cite{liu2015deep}, using FID~\cite{heusel2017gans}.
\subsection{Log-likelihood Evaluatoin}
\begin{table}[tb]
\centering
\begin{tabular}{lrr}  
\toprule
Model  &  MNIST & CIFAR\\
\midrule
\textbf{With autoregressive}   \\
PixelCNN         &  81.30  &  3.14   \\
DRAW             &  80.97  &  3.58    \\
IAFVAE           &  79.88  &  3.11    \\
PixelVAE++       &  78.00  &  2.90   \\
PixelRNN         &  79.20  &  3.00    \\
VLAE             &  79.03  &  2.95     \\
PixelSNAIL       &         & 2.85      \\
PixelHVAE with VampPrior &  78.45  &     \\
\midrule
\textbf{Without autoregressive}   \\
Implicit Optimal Priors & 83.21 \\
Discrete VAE     &  81.01     \\
LARS             &  80.30     \\
VampPrior        &  79.75     \\
BIVA            &  78.59      &    3.08    \\
\textbf{Naive VAEPP}      &  76.49 & 3.15    \\
\textbf{VAEPP}            &  76.37 & 2.91	    \\
\textbf{VAEPP+Flow}       &  76.23 & 2.84    \\
\bottomrule
\end{tabular}
\caption{Test NLL on MNIST and Bits/dim on CIFAR-10. Bits/dim means $-\log p_\theta(x|z) / (3072 * \ln(2))$. The data are from \protect\cite{maaloe2019biva},  \protect\cite{chen2018pixelsnail}, \protect\cite{tomczak2018vae}, \protect\cite{bauer2019resampled} and \protect\cite{takahashi2019variational}. 
%The learnable prior is indeed more useful on MNIST since the reconstruction loss is not hard to optimize on MNIST and the improvement by autoregressive component is limited. On CIFAR-10, the image is more complex and reconstruction loss is hard to optimize. Therefore, autoregressive component improve VAEPP significantly and the improvement of learnable prior is limited. 
VAEPP+Flow means VAEPP with a normalization flow on encoder, to enrich  encoder. 
Additional, we compare VAE based on $q_\phi(z|x)$ and $q_\phi(z|e)$ on MNIST, whose NLL are 81.10 and 83.30 respectively. Moreover, evaluation using importance sampling based on $q_\phi(z|e)$ has enough small standard deviation (0.01) with $10^8$ samples altogether. It validates that $q_\phi(z|e)$ is stable for evaluation and doesn't improve the performance. VAEPP reaches SOTA without autoregressive component, and is comparable to models with autoregressive component. }
\label{tab:mnist-nll}
\end{table}
\begin{table}[tb]
\centering
\begin{tabular}{lrrr}  
\toprule
Model   & Static MNIST & Fashion & Omniglot \\
\midrule
Naive VAEPP    &   78.06   &  214.63  &   90.72 \\
VAEPP          &   77.73   &  213.24   &   89.60  \\
VAEPP+Flow     &   77.66   &  213.19  &   89.24  \\
\bottomrule
\end{tabular}
%Naive VAEPP    &  81.16$\pm$    &  225.08$\pm$1.18  &  96.86$\pm$  \\
%VAEPP          &  79.85$\pm$    &  222.42$\pm$1.57  &  88.85$\pm$   \\
%VAEPP+Flow     &  80.77$\pm$    &  222.23$\pm$1.39  &  88.83$\pm$   \\
\caption{Test NLL on Static MNIST, Fashion-MNIST and Omniglot.  }
\label{tab:cifar-nll}
\end{table}
We compare our algorithms with other models based on log-likelihood, on MNIST and CIFAR-10 as shown in \cref{tab:mnist-nll}, and on Static-MNIST, Fashion-MNIST, and Omniglot, as shown in \cref{tab:cifar-nll}. Because the improvement of auto-regressive components is significant, we separate models by whether they use an auto-regressive component. VAEPP outperforms most of the models without autoregressive component and is comparable to the models with autoregressive component. The reason of why VAEPP doesn't use an auto-regressive component is that VAEPP is time-consuming in training,  evaluation and sampling due to the huge structure (need additional discriminator) and Langevin Dynamics. It is not easy to apply an auto-regressive component on VAEPP since auto-regressive component is also time-consuming. 
% Concretely, a whole process including training, evaluation and sampling on CIFAR10 will cost roughly one week on single Nvidia 2080Ti GPU card. 
%On the other hand, we expect that the pure improvement on learnable prior could improve the performance of VAE rather than the careful design on encoder or decoder, since it is clearer and easier to develop in theory. 
Therefore, how to apply an autoregressive component on VAEPP is a valuable and challenging practical work and we leave it for future work.

We evaluate and compare the performance of VAEPP trained by \cref{alg:vaepp} and the Naive VAEPP trained by \cref{alg:improved_vaepp} on CIFAR10, as the gradient penalty algorithm is chosen from 3 strategies: WGAN-GP, WGAN-div-1 (sampling the linear interpolation of  real data and generated data) and WGAN-div-2 (sampling real data and generated data), as shown in \cref{tab:compare_nD_over_R}. 
%Our conclusion is that \cref{alg:improved_vaepp} outperforms \cref{alg:vaepp} under all  settings in CIFAR-10 and we select WGAN-div-1 as the default setting.
\begin{table}[tb]
\centering
\begin{tabular}{lrr}  
\toprule
GP Strategy  &  Naive VAEPP  & VAEPP \\
\midrule
WGAN-GP      &  3.15   & 2.95      \\
WGAN-div-1   &  3.20   & 2.91      \\
WGAN-div-2   &  4.47   & 2.99      \\
\bottomrule
\end{tabular}
\caption{Comparation between VAEPP and Improved VAEPP when gradient penalty strategy varies on CIFAR-10 with dim $\mathcal{Z} = 1024$. For any gradient penalty strategy in the table, VAEPP outperforms Naive VAEPP, which validates the our intuition of design of \cref{alg:improved_vaepp}.  WGAN-div-1 is chosen as our default gradient penalty strategy since it reaches best performance in VAEPP. 
%Although WGAN-GP outperforms WGAN-div-1 in Naive VAEPP, the difference is acceptable (3.20 and 3.15 is both not good enough) and Naive VAEPP is not the final algorithm. 
}
\label{tab:compare_nD_over_R}
\end{table}

To validate that it is better to use $q_\phi(z)$ to evaluate $Z$ than $p_\mathcal{N}(z)$ in \cref{subsec:determine_z}, we calculate the $KL(q_\phi(z)||p_\lambda(z))$ and $KL(p_\mathcal{N}(z)||p_\lambda(z))$ on CIFAR-10 and MNIST. The former is less than $\mathcal{L}-\mathcal{I}$~\cite{hoffman2016elbo}(180.3 on CIFAR-10 and 12.497 on MNIST), and the latter can be evaluated directly (1011.30 on CIFAR-10 and 57.45 on MNIST). Consequently, $q_\phi(z)$ is much closer to $p_\lambda(z)$ than $p_\mathcal{N}(z)$. 

To ensure the variance of estimation $\hat{Z}$ is small enough, the $q_\phi(z|e)$ is chosen as truncated normal distribution (drop the sample whose magnitude is more than 2 standard deviation form the mean) instead of normal distribution. In \cref{eq:Z_estimator}, $\hat{q}_\phi(z)$ estimated by $\frac{1}{N}q_\phi(z|e^{(j)})$ is the denominator and if it becomes too small sometimes, the estimation will become unstable. If $q_\phi(z|e^{(j)})$ is chosen as normal distribution, the probability of $z$ in tail will be larger when the number of samples become larger, which will lead to huge variance to the estimation $\hat{Z}$. With $10^9$ samples, the variance of these two method are $0.809260$ (normal) and $0.000967$ (truncated normal) in MNIST. Therefore, truncated normal is chosen as default setting. 

%To validate that VAEPP is more stable and efficient than Naive VAEPP in \cref{subsec:improve_of_vaepp}, we draw the training loss of VAEPP and Naive VAEPP on CIFAR-10, shown in \cref{fig:loss_curves}.
% We ensure the variance of results shown in \cref{tab:cifar-nll} and \cref{tab:mnist-nll} are less than 0.05. 
%We evaluate $Z$ by two ways mentioned in \cref{subsec:determine_z} and compare the variance of them in 10 times on CIFAR-10. The variance of estimation based on $q_\phi(z)$ (4.82), is much less than the variance of estimation based on $p_\mathcal{N}$ (TODO), which supports our proposition in \cref{subsec:determine_z}. Since the variance of $Z$ also influence the variance of log-likelihood, we also evaluate the variance of log-likelihood in 10 times on CIFAR-10 (4.82) to ensure the stability of evaluation. 

\subsection{Quality of Sampling}
As common sense, the quality of sampling of VAEs is worse than GANs, and it is indeed a reason that we involve the techniques of GAN to improve VAE model: We use the discriminator to adjust learnable prior and a GAN to sample the initial $z_0$ for Langevin Dynamics. These techniques will help VAEPP improve the quality of samples. The samples of VAEPP gets good FID, comparable to GANs and 2-Stage VAE (which is the SOTA of VAE in FID), as shown in \cref{tab:compare_FID}. Some generated images are shown in \cref{fig:show_images}.
\begin{figure}[tb]
	\centering
	\includegraphics[width=1.0\columnwidth]{../figures/celeba}
	\includegraphics[width=1.0\columnwidth]{../figures/cifar}
	\caption{
		Examples of generated images.
	}
	\label{fig:show_images}
\end{figure}
\begin{table}[tb]
\centering
\begin{tabular}{lrrrrrrr}  
\toprule
Model & MNIST & Fashion & CIFAR & CelebA\\
\midrule
Best GAN   & $\sim10$& $\sim32$&$\sim70$& $\sim49$\\
VAE+Flow   & $54.8$  & $62.1$  & $81.2$ & $65.7$\\
WAE-MMD    & $115.0$ & $101.7$ & $80.9$ & $62.9$\\
2-StageVAE & $12.6$  & $29.3$  & $72.9$ & $44.4$\\
GAN-VAEPP  & $12.7$  & $26.4$  & $74.1$ & $53.4$ \\
VAEPP      & $12.0$  & $26.4$  & $71.0$ & $53.4$ \\
\bottomrule
\end{tabular} 
\caption{FID comparison of GAN-based models and other VAEs. Best GAN indicates the best FID on each dataset across all GAN models when trained using settings suggested by original authors. VAEPP uses Bernouli as posterior on MNIST and Discretized Logistic~\protect\cite{salimans2017pixelcnn++} on others. GAN-VAEPP indicates that image is directly sampled from $z_0$, generated by GAN without Langevin Dynamics. The data of Best GAN and other VAEs is from \protect\cite{dai2019diagnosing}. The FID of VAEPP is usually better than GAN-VAEPP 
%(except Fashion, the texture is hard to reconstruct, which leads to the reconstruction is much different to real image. It will misguide the Langevin Dynamics since we have assumed that)
, which validates that the explicit prior and Langevin Dynamics are useful for improving the quality of sampling.
}\label{tab:compare_FID}
\end{table}

It is hard to reach best FID, IS~\cite{salimans2016improved} and log-likelihood simultaneously with the same setting. We observe the fact that when $\dim \mathcal{Z}$ (the dimension of latent space) increases, the trends of FID and IS are greatly different to log-likelihood's, as shown in \cref{fig:fid_different_dim}. As diagnosis in \cite{dai2019diagnosing}, the variance of $p_\theta(x|z)$ is chosen as a learnable scalar $\gamma$, and the $\dim \mathcal{Z}$ is chosen as a number, a little larger than the dimension of real data manifold, $\dim \mathcal{Z} = 128$, as our experimental result.  

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\columnwidth]{../dist.strip/z_dim}
	\caption{
	Comparison of VAEPP with a learnable scalar $\gamma$ (variance of $p_\theta(x|z)$), as the dimension of latent space varies on CIFAR-10, with metrics BPD, FID and IS. FID and BPD is better when it is smaller and IS is better when it is larger. When $\dim \mathcal{Z}$ is greater than 128, the quality of sampling becomes worse and BPD becomes better as $\dim \mathcal{Z}$ increases. It validates the proposition that $\dim \mathcal{Z}$ should be chosen as a minimal number of active latent dimensions in \protect\cite{dai2019diagnosing}. The reconstruction term is optimized more as $\dim \mathcal{Z}$ increases, because larger latent space could keep more information. Meanwhile, the $KL(q_\phi(z)||p_\lambda(z))$ (Bounded by the difference between ELBO and reconstruction term) increases not much since the learnable prior could relieve the increasing of it. It also shows an interesting phenomenon that the  trends of FID and IS, are not always same as BPD, maybe greatly different. 
	}
	\label{fig:fid_different_dim}
\end{figure}

For better understanding, the value of discriminator in this section is normalized into $\mathcal{N}(0, 1)$ in training set.

To validate the \cref{eq:behavior_of_beta}, we calculate the $\E_{p_\lambda(z)}[ D(G(z))]$ (discriminator on generated samples) and $\E_{q_\phi(z)}[ D(G(z))]$ (discriminator on reconstructed samples). They are 0.092 and 0.015 respectively on CIFAR-10, which means discriminator on generated samples and reconstructed samples are nearly same as the discriminator on real data. 
% The mean and std of energy in training set is -19.986605, 2.5140648
% The discriminator on generated samples is -19.75406244, std is 0.3330956099129594
% The discriminator on reconstructed samples is -19.9492;

To validate the assumption introduced in \cref{subsec:inference} indeed holds in practical experiment, $|\E_{p_\theta(x|z)}D(x) - D(G(z))|$ is calculated and it is an acceptable value (0.019) on CIFAR-10. 

% CIFAR [-12.352115, -12.347757, -12.345433, -12.345455, -12.3524475, 
% -12.345604, -12.345031, -12.348698, -12.346703, -12.350895, -12.353839,
% -12.3432255, -12.342315, -12.346496, -12.341743, -12.346189, -12.354458, 
% -12.340977, -12.341134, -12.343964, -12.348272, -12.349046, -12.338009,
% -12.350307, -12.359574, -12.354169, -12.354169, -12.341797, -12.34804,
% -12.349505, -12.348672, -12.350969, -12.34367, -12.34062, -12.347913,
% -12.340741, -12.340741, -12.343849, -12.337844, -12.339806, -12.347616,
% -12.347033, -12.347033, -12.347033, -12.34675, -12.346854, -12.349438,
% -12.351765, -12.348437, -12.3403635, ] 
% http://mlserver.ipwx.me:7897/5e0a3a1642d74cd7474dc164/



% \subsection{Out-of-Distribution}

%We found that discriminator of VAEPP performs normally while model assign higher density to the data from out-of-distribution, as shown in fig(TODO). It inspires us to use the exact information from discriminator to design 3 indicators, log-likelihood $\log p_\theta(x)$, energy $D(x)$ and norm of gradient $\|\nabla_{x} D(x)\|$ to solve OoD problem, as shown in fig(). From the likelihood, \cite{song2017pixeldefend} showed that it is useful to assume the data is out-of-distribution if it have very high or very low density. For energy and norm of gradient, it performs normally, i.e., when energy is higher or norm of gradient is smaller, it more likely belongs to out-of-distribution. Hence, we evaluate the AUC and AP for them and combination of them in OoD problem on CIFAR and SVHN, as shown in \cref{tab:compare_ood}. The method to combine them is simple: normalize them into a standard Gaussian in training set of CIFAR, and the score of this indicator contributing to combination score is the absolute/original/negative value of it. We have tested all kinds of combination and the best one is $|\log p_\theta(x)| - \|\nabla_x D(x)\|$. The performance of it on other datasets is shown in \cref{tab:compare_ood_other_datasets} which is competitive to other indicators $T_{perm}$~\cite{song2017pixeldefend} and WAIC~\cite{choi2018waic}. 
%\begin{table}[tb]
%\centering
%\begin{tabular}{lrr}  
%\toprule
%Indicator  & AUC/AP \\
%\midrule
%$\log p_\theta(x)$   &  1.00/1.00      \\
%$D(x)$               &  1.00/1.00      \\
%$\|\nabla_x D(x)\|$  &  1.00/1.00      \\
%$|\log p_\theta(x)| - \|\nabla_x D(x)\|$ & 1.00/1.00 \\
%\bottomrule
%\end{tabular}
%\caption{Comparation between indicators and their combination.}
%\label{tab:compare_ood}
%\end{table}
%\begin{table}[tb]
%\centering
%\begin{tabular}{lrrrr}  
%\toprule
%Dataset  &  $T_{perm}$ & WAIC & VAEPP\\
%\midrule
%Fashion vs MNIST  & 0.78/0.71  & 0.24/0.38 & 1.00/1.00  \\
%CIFAR vs SVHN     & 0.86/0.82  & 0.16/0.55 & 1.00/1.00  \\
%CIFAR vs ImageNet & 0.50/0.51  & 0.58/0.59 & 1.00/1.00  \\
%CIFAR vs LSUN     & 0.58/0.56  & 0.60/0.28 & 1.00/1.00  \\
%\bottomrule
%\end{tabular}
%\caption{Comparation the performance of VAEPP with other models in OoD problem. The data of $T_{perm}$ and WAIC is from \protect \cite{song2019unsupervised}. The reason that we can't compare VAEPP with \protect\cite{song2019unsupervised} is that it assumes a batch of OoD data can be obtained but we don't have this assumption. In fast, many online anomaly detection system must deal the online data on time when the number of OoD data in a batch is unknown. }
%\label{tab:compare_ood_other_datasets}
%\end{table}

