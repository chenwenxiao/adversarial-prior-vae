\section{VAEPP}\label{sec:vaepp}


In \cref{sec:pull_back_prior}, we focus on the 2nd optimization and propose the Pull-back Prior as the analytical solution of it. In this section, we will return to the original objective ELBO, and discuss how to use Pull-back Prior to optimize ELBO. Finally, we will show the architecture of VAE with Pull-back Prior. 

\subsection{Determine $\beta$}

$\beta$ in \cref{eq:pull_back_prior} represents how far $p_\lambda$ is from $p_\mathcal{N}$, but how we decide the value of $\beta$? When $\beta$ is smaller, the difference between $p_\lambda$ and $p_\mathcal{N}$ is less, i.e. the representation ability of $p_\lambda$ is severely limited. When $\beta$ is larger, $p_\lambda$ is farther from $p_\mathcal{N}$. But noticing that in \cref{eq:final_optimization}, we simplify the optimization of $D$ by a fixed $D$ obtained in $W^1(\E_{p_\mathcal{N}}p_\theta(x|z), p^*)$, if $p_\lambda$ is enough far from $p_\mathcal{N}$, this approximation will become invalid. Consequently, $\beta$ should be set to an appropriate value which can't limit the representation ability of $p_\lambda$ and could make sure the approximation $D$ is valid. It is important to realize that the Pull-back Prior is serving for better ELBO. Whatever the representation ability of $p_\lambda$ is limited or approximation $D$ is invalid, the ELBO will suffer. Therefore, it is reasonable to set $\beta$ by the optimization for ELBO ($\lambda$ contains $\beta$ and $\omega$ which is the parameters of $D$):
\begin{equation}
	\beta = \arg \min_{\beta} \mathcal{L}(\theta, \phi, \lambda) = \arg \min_{\beta} \mathcal{L}(\theta, \phi, \beta, \omega) \tag{8}
\end{equation}

Afterwards, let we research the behavior of $\beta$ during training, i.e. the sign of $\partial \mathcal{L}/\partial \beta$.
\begin{align*}\label{eq:behavior_of_beta}
\frac{\partial \mathcal{L}}{\partial \beta} &= \E_{q_\phi(z)}[ -D(G(z))] - \frac{\partial Z}{\partial \beta} \\
&= \E_{p_\lambda(z)}[ D(G(z))] - \E_{q_\phi(z)}[ D(G(z))]  \tag{9}
\end{align*}
The 1st term in \cref{eq:behavior_of_beta} is the mean of discriminator on data generated from $p_\lambda$. The 2nd term in \cref{eq:behavior_of_beta} is the mean of discriminator on reconstructed data which is near same as real data when reconstruction is well-trained. Hence, $\partial \mathcal{L}/\partial \beta = 0$ represents that the discriminator can't distinguish the reconstructed data (near same as real data) and generated data. It coincides the philosophy of GAN. 

\subsection{Determine $Z$}

We have known that $Z = \int_{\mathcal{Z}} p_\mathcal{N}(z) \exp\{- \beta * D(G(z))\} \dd z$, denoted by $\int_{\mathcal{Z}} f_\lambda(z) \dd z$. It is natural to determine $Z$ by importance sampling $Z = \E_{p_\mathcal{N}(z)} \exp\{- \beta * D(G(z))\}$ as [] did. By the theory of importance sampling, the variance of the estimation of $Z$ is $\frac{1}{M} Var_{p_b}[\frac{f_\lambda}{p_b}]$ where $M$ is the number of samples. and hence the variance will be larger when $f_\lambda$ is farther from $p_b$. If we choose $p_b$ as $p_\mathcal{N}$, when $\beta$ is large, the variance will be larger and it will influence the optimization and evaluation. 

The optimal choice for $p_b$ is $p_\lambda$ itself but it is hard to sample from $p_\lambda$ during training. We try to find a distribution which is near to $p_\lambda$ and easy-sampling. As \cref{eq:behavior_of_beta} shows, when $\beta$ approaches optimal, the discriminator can't distinguish the data generated by $p_\lambda(z)$ and $q_\phi(z)$.  \cref{eq:second-decomposition} also shows that when $p_\lambda(z)$ is optimized for $\mathcal{L}(\theta, \phi, \lambda)$, it approaches to $q_\phi$. Consequently, it is reasonable to choose $q_\phi$ as $p_b$. 

However, as we mentioned before, $q_\phi(z)$ is intractable to computing the exact density. We introduce a bias estimation for $q_\phi(z)$, which will lead to the bias estimation for $Z$. 
\begin{equation*}
	q_\phi(z) = \E_{p^*(x)} q_\phi(z|x) \approx \frac{1}{N}\sum_{i=1}^N q_\phi(z|x^{(i)}) \geq \frac{1}{N} q_\phi(z|x^{(j)})
\end{equation*}
where $x^{(j)}$ is one of real data, $N$ is the size of training set. To reduce the error, $q_\phi(z|x^{(j)})$ should be one of the largest in summation. Therefore, we firstly choose $x^{(j)}$, then sample $z$ from $q_\phi(z|x^{(j)})$ (by this way, $q_\phi(z|x^{(j)})$ will be large enough), and finally set $\frac{1}{N} q_\phi(z|x^{(j)})$ as a bias estimation for $q_\phi(z)$. When $p^*(x)$ consists of infinite data, \EG in MNIST the input of model is sampled from real images, $p^*(x)$ is sampled from a finite set $\{e^{(1)}, \ldots, e^{(1)}\}$. The estimation is:
\begin{equation*}
	q_\phi(z) \approx \frac{1}{N}\sum_{i=1}^N \E_{p^*(x|e^{(j)})} q_\phi(z|x) \geq \frac{1}{N} \E_{p^*(x|e^{(j)})} q_\phi(z|x)
\end{equation*} 
where $p^*(x|e)$ means the sampling process from $e$. Since it is near same as above, and following theorem holds for both, we will use the first estimation through this paper. 

After that, we could get a bias estimation $\hat{Z}$ for $Z$:
\begin{align*}\label{eq:Z_estimator}
	Z = \E_{q_\phi(z)}\frac{f_\lambda(z)}{q_\phi(z)} \leq \E_{p*(x)}\E_{q_\phi(z|x)} N \frac{f_\lambda(z)}{q_\phi(z|x)} = \hat{Z} \tag{10}
\end{align*}

Because $\beta$ is optimized from small to large during training, we use both estimations for $Z$ in training. After training, $\beta$ is large and $p_\lambda$ approach to $q_\phi$ by \cref{eq:behavior_of_beta}, therefore we use \cref{eq:Z_estimator} for computing the final value of $Z$. The last thing we need to check is that the bias of estimation will not improve the log-likelihood in evaluation:
\begin{equation*}
	p_\theta(x) = \int_{\mathcal{Z}} \frac{1}{Z} f_\lambda(z) p_\theta(x|z) \geq \int_{\mathcal{Z}} \frac{1}{\hat{Z}} f_\lambda(z) p_\theta(x|z) = \hat{p}_\theta(x)
\end{equation*}
which means the $\hat{p}_\theta(x)$ is a lower bound of real model density $p_\theta(x)$.  

Thanks to the stable and efficient gradient penalty regularizer term provided by WGAN-GP and WGAN-div, we enjoy stable and powerful training process. Training algorithm for VAEPP based on WGAN-GP is provided as \cref{alg:vaepp}:
\begin{algorithm}[tb]
\caption{VAEPP training algorithm}
\label{alg:vaepp}
\textbf{Require}: The gradient penalty algorithm $R$, the number of critic iterations $n_c$, the batch size $b$. The parameters for Adam Optimizers, $\tau$. 
\begin{algorithmic}[1] %[1] enables line numbers
\WHILE{$\theta, \phi, \beta, \omega$ have not converged}
\FOR {$t = 1, \ldots, n_c$}
\FOR {$i = 1, \ldots, b$}
\STATE Sample real data $x \sim p^*$, latent variable $z \sim p_\mathcal{N}$.

\STATE	$\hat{x} = \E_{p_\theta(x|z)}[x]$
\STATE Get gradient penalty $\zeta \gets R(x, \hat{x})$. 
\STATE $L^{(i)} \gets D(\hat{x}) - D(x) + \zeta$
\ENDFOR
\STATE $\omega \gets $ Adam $(\nabla_{\omega} \frac{1}{b}\sum_{i}^b L^{(i)}, \omega, \tau)$
\ENDFOR
\FOR {$i = 1, \ldots, b$}
\STATE Sample real data $x \sim p^*$, $z \sim q_\phi(z|x)$, $\epsilon \sim p_\mathcal{N}$.
\STATE $Z^{(i)} \gets \frac{1}{2}(\exp\{-\beta * D(G(\epsilon))\} + N \frac{f_\lambda(z)}{q_\phi(z|x)})$
\STATE $\mathcal{L}^{(i)} \gets \log p_\theta(x|z) + \log f_\lambda(z) - \log q_\phi(z|x)$
\ENDFOR
\STATE $\mathcal{L} \gets \frac{1}{b}\sum_{i}^b \mathcal{L}^{(i)} - \log (\frac{1}{b}\sum_{i}^b Z^{(i)})$
\STATE $\theta, \phi, \beta \gets $ Adam $(\nabla_{\theta, \phi, \beta} \mathcal{L}, \{\theta, \phi, \beta\}, \tau)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

 