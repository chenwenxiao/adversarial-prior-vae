\section{VAEPP}\label{sec:vaepp}


In \cref{sec:pull_back_prior}, we focus on the 2nd optimization and propose the Pull-back Prior as the analytical solution of it. In this section, we will return to the original objective ELBO, and discuss how to use Pull-back Prior to optimize ELBO. Finally, we will show the architecture of VAE with Pull-back Prior. 

\subsection{Determine $\beta$}

$\beta$ in \cref{eq:pull_back_prior} represents how far $p_\lambda$ is from $p_\mathcal{N}$, but how we decide the value of $\beta$? When $\beta$ is smaller, the difference between $p_\lambda$ and $p_\mathcal{N}$ is less, i.e. the representation ability of $p_\lambda$ is severely limited. When $\beta$ is larger, $p_\lambda$ is farther from $p_\mathcal{N}$. But noticing that in \cref{eq:final_optimization}, we simplify the optimization of $D$ by a fixed $D$ obtained in $W^1(\E_{p_\mathcal{N}}p_\theta(x|z), p^*)$, if $p_\lambda$ is enough far from $p_\mathcal{N}$, this approximation will become invalid. Consequently, $\beta$ should be set to an appropriate value which can't limit the representation ability of $p_\lambda$ and could make sure the approximation $D$ is valid. It is important to realize that the Pull-back Prior is serving for better ELBO. Whatever the representation ability of $p_\lambda$ is limited or approximation $D$ is invalid, the ELBO will suffer. Therefore, it is reasonable to set $\beta$ by the optimization for ELBO ($\lambda$ contains $\beta$ and $\omega$ which is the parameters of $D$):
\begin{equation}
	\beta = \arg \min_{\beta} \mathcal{L}(\theta, \phi, \lambda) = \arg \min_{\beta} \mathcal{L}(\theta, \phi, \beta, \omega) \tag{8}
\end{equation}

Afterwards, let we research the behavior of $\beta$ during training, i.e. the sign of $\partial \mathcal{L}/\partial \beta$.
\begin{align*}\label{eq:behavior_of_beta}
\frac{\partial \mathcal{L}}{\partial \beta} &= \E_{q_\phi(z)}[ -D(G(z))] - \frac{\partial Z}{\partial \beta} \\
&= \E_{p_\lambda(z)}[ D(G(z))] - \E_{q_\phi(z)}[ D(G(z))]  \tag{9}
\end{align*}
The 1st term in \cref{eq:behavior_of_beta} is the mean of discriminator on data generated from $p_\lambda$. The 2nd term in \cref{eq:behavior_of_beta} is the mean of discriminator on reconstructed data which is near same as real data when reconstruction is well-trained. Hence, $\partial \mathcal{L}/\partial \beta = 0$ represents that the discriminator can't distinguish the reconstructed data (near same as real data) and generated data. It coincides the philosophy of GAN. 

\subsection{Determine $Z$}

We have known that $Z = \int_{\mathcal{Z}} p_\mathcal{N}(z) \exp\{- \beta * D(G(z))\} \dd z$, denoted by $\int_{\mathcal{Z}} f_\lambda(z) \dd z$. It is natural to determine $Z$ by importance sampling $Z = \E_{p_\mathcal{N}(z)} \exp\{- \beta * D(G(z))\}$ as [] did. By the theory of importance sampling, the variance of the estimation of $Z$ is $\frac{1}{M} Var_{p_b}[\frac{f_\lambda}{p_b}]$ where $M$ is the number of samples. and hence the variance will be larger when $f_\lambda$ is farther from $p_b$. If we choose $p_b$ as $p_\mathcal{N}$, when $\beta$ is large, the variance will be larger and it will influence the optimization and evaluation. 

The optimal choice for $p_b$ is $p_\lambda$ itself but it is hard to sample from $p_\lambda$ during training. We try to find a distribution which is near to $p_\lambda$ and easy-sampling. As \cref{eq:behavior_of_beta} shows, when $\beta$ approaches optimal, the discriminator can't distinguish the data generated by $p_\lambda(z)$ and $q_\phi(z)$.  \cref{eq:second-decomposition} also shows that when $p_\lambda(z)$ is optimized for $\mathcal{L}(\theta, \phi, \lambda)$, it approaches to $q_\phi$. Consequently, it is reasonable to choose $q_\phi$ as $p_b$. 

However, as we mentioned before, $q_\phi(z)$ is intractable to computing the exact density. We introduce a bias estimation for $q_\phi(z)$, which will lead to the bias estimation for $Z$. 
\begin{equation*}
	q_\phi(z) = \E_{p^*(x)} q_\phi(z|x) \approx \frac{1}{N}\sum_{i=1}^N q_\phi(z|x^{(i)}) \geq \frac{1}{N} q_\phi(z|x^{(j)})
\end{equation*}
where $x^{(j)}$ is one of real data, $N$ is the size of training set. To reduce the error, $q_\phi(z|x^{(j)})$ should be one of the largest in summation. Therefore, we firstly choose $x^{(j)}$, then sample $z$ from $q_\phi(z|x^{(j)})$ (by this way, $q_\phi(z|x^{(j)})$ will be large enough), and finally set $\frac{1}{N} q_\phi(z|x^{(j)})$ as a bias estimation for $q_\phi(z)$. When $p^*(x)$ consists of infinite data, \EG in MNIST the input of model is sampled from real images, $p^*(x)$ is sampled from a finite set $\{e^{(1)}, \ldots, e^{(1)}\}$. The estimation is:
\begin{equation*}
	q_\phi(z) \approx \frac{1}{N}\sum_{i=1}^N \E_{p^*(x|e^{(j)})} q_\phi(z|x) \geq \frac{1}{N} \E_{p^*(x|e^{(j)})} q_\phi(z|x)
\end{equation*} 
where $p^*(x|e)$ means the sampling process from $e$. Since it is near same as above, and following theorem holds for both, we will use the first estimation through this paper. 

After that, we could get a bias estimation $\hat{Z}$ for $Z$:
\begin{align*}\label{eq:Z_estimator}
	Z = \E_{q_\phi(z)}\frac{f_\lambda(z)}{q_\phi(z)} \leq \E_{p*(x)}\E_{q_\phi(z|x)} N \frac{f_\lambda(z)}{q_\phi(z|x)} = \hat{Z} \tag{10}
\end{align*}

Because $\beta$ is optimized from small to large during training, we use both estimations for $Z$ in training. After training, $\beta$ is large and $p_\lambda$ approach to $q_\phi$ by \cref{eq:behavior_of_beta}, therefore we use \cref{eq:Z_estimator} for computing the final value of $Z$. The last thing we need to check is that the bias of estimation will not improve the log-likelihood in evaluation:
\begin{equation*}
	p_\theta(x) = \int_{\mathcal{Z}} \frac{1}{Z} f_\lambda(z) p_\theta(x|z) \geq \int_{\mathcal{Z}} \frac{1}{\hat{Z}} f_\lambda(z) p_\theta(x|z) = \hat{p}_\theta(x)
\end{equation*}
which means the $\hat{p}_\theta(x)$ is a lower bound of real model density $p_\theta(x)$.  

Thanks to the stable and efficient gradient penalty regularizer term provided by WGAN-GP and WGAN-div, we enjoy stable and powerful training process. Training algorithm for VAEPP based on WGAN-GP is provided as \cref{alg:vaepp}. We remove $n_{critic}$, number of critic iterations per generator iteration, from WGAN-GP framework since there is no significant difference when $n_{critic}$ varies in VAEPP.
\begin{algorithm}[tb]
\caption{VAEPP training algorithm}
\label{alg:vaepp}
\textbf{Require}: The gradient penalty algorithm $R$, the batch size $b$, the parameters for Adam Optimizers, $\tau$. 

\begin{algorithmic}[1] %[1] enables line numbers
\WHILE{$\theta, \phi, \beta, \omega$ have not converged}
\FOR {$i = 1, \ldots, b$}
\STATE Sample real data $x \sim p^*$, latent variable $z \sim p_\mathcal{N}$.
\STATE	$\hat{x} = \E_{p_\theta(x|z)}[x]$, get gradient penalty $\zeta \gets R(x, \hat{x})$. 
\STATE $L^{(i)} \gets D(\hat{x}) - D(x) + \zeta$
\ENDFOR
\STATE $\omega \gets $ Adam $(\nabla_{\omega} \frac{1}{b}\sum_{i}^b L^{(i)}, \omega, \tau)$
\FOR {$i = 1, \ldots, b$}
\STATE Sample real data $x \sim p^*$, $z \sim q_\phi(z|x)$, $\epsilon \sim p_\mathcal{N}$.
\STATE $Z^{(i)} \gets \frac{1}{2}(\exp\{-\beta * D(G(\epsilon))\} + N \frac{f_\lambda(z)}{q_\phi(z|x)})$
\STATE $\mathcal{L}^{(i)} \gets \ln p_\theta(x|z) + \ln f_\lambda(z) - \ln q_\phi(z|x)$
\ENDFOR
\STATE $\mathcal{L} \gets \frac{1}{b}\sum_{i}^b \mathcal{L}^{(i)} - \ln (\frac{1}{b}\sum_{i}^b Z^{(i)})$
\STATE $\theta, \phi, \beta \gets $ Adam $(\nabla_{\theta, \phi, \beta} \mathcal{L}, \{\theta, \phi, \beta\}, \tau)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Improvement of VAEPP}
We have introduced the architecture and training algorithm for VAEPP, whose performance is better than vanilla VAE and some variants of VAE with learnable prior. However, we notice that in the training process, the optimization of $\omega$ may influence the optimization of $\theta, \phi, \beta$, \EG the optimization for $\omega$ significantly worsen the loss function which have been optimized well. The reason is that the optimization for $\omega$ is independent to the optimization of $\theta, \phi, \beta$ in \cref{alg:vaepp}. This independence is from the philosophy of GAN but it may suffer the performance of VAEPP (log-likelihood). Hence, is it possible to combine this two optimization into one to improve the performance and stability of VAEPP? Our answer is positive and our goal is to use SGVB with gradient penalty regularizer to train it. 

Let's see what will happen if $\mathcal{L}(\theta, \phi, \beta, \omega)$ is directly trained by SGVB. The behavior of $\theta, \phi, \beta$ will be same as \cref{alg:vaepp} since the optimization for them is not changed. The only thing we need to check is the optimization for $\max_{Lip(D) \leq 1} \mathcal{L}(\theta, \phi, \beta, \omega)$ (sign $\simeq$ means that there is only a constant difference between left and right). We firstly show a inequality of $\ln Z$:
\begin{equation*}
	\ln Z = \ln \E_{p_\mathcal{N}} \exp \{- \beta * D(G(z))\} \geq \E_{p_\mathcal{N}} [- \beta * D(G(z))]
\end{equation*}
Then $\max_{Lip(D) \leq 1} \mathcal{L}(\theta, \phi, \beta, \omega)$ indeed find a suboptimal solution for $W^1(p^*, p_\theta)$. 
\begin{align*}\label{eq:improve_vaepp}
	\max_{Lip(D) \leq 1} \mathcal{L} &\simeq \max_{Lip(D) \leq 1} \{ -\E_{q_\phi(z)} \beta*D(G(z)) - \ln Z \}\\ 
	&\leq \beta \max_{Lip(D) \leq 1} \{ \E_{p_\mathcal{N}} D(G(z)) - E_{q_\phi(z)} D(G(z)) \} \\
	&= \beta W^1(p_\theta, p_r) \approx \beta W^1(p_\theta, p^*) \tag{11}
\end{align*}
where $p_r$ denotes $p_r(x) = \E_{q_\phi(z)} p_\theta(x|z)$, consisting of reconstructed data. The last approximation sign is from the fact that $p_\theta \rightarrow p^*$ after a few epoch in the training of VAE. 

\cref{eq:improve_vaepp} indicates that is is possible to obtain a suboptimal solution for $D$ by directly optimizing $\mathcal{L}$ and the gradient penalty term should be multiplied by $\beta$. By this way, the optimizations for $\omega$ and $\theta, \phi, \beta$ is combined into one, which is provides as \cref{alg:improved_vaepp}. 

\begin{algorithm}[tb]
\caption{Improved VAEPP training algorithm}
\label{alg:improved_vaepp}
\textbf{Require}: The gradient penalty algorithm $R$, the batch size $b$, the parameters for Adam Optimizers, $\tau$. 

\begin{algorithmic}[1] %[1] enables line numbers
\WHILE{$\theta, \phi, \beta, \omega$ have not converged}
\FOR {$i = 1, \ldots, b$}
\STATE Sample real data $x \sim p^*$, $z \sim q_\phi(z|x)$, $\epsilon \sim p_\mathcal{N}$.
\STATE $\hat{x} = \E_{p_\theta(x|z)}[x]$, get gradient penalty $\zeta \gets R(x, \hat{x})$. 
\STATE $Z^{(i)} \gets \frac{1}{2}(\exp\{-\beta * D(G(\epsilon))\} + N \frac{f_\lambda(z)}{q_\phi(z|x)})$
\STATE $\mathcal{L}^{(i)} \gets \ln p_\theta(x|z) + \ln f_\lambda(z) - \ln q_\phi(z|x) + \beta \zeta$
\ENDFOR
\STATE $\mathcal{L} \gets \frac{1}{b}\sum_{i}^b \mathcal{L}^{(i)} - \ln (\frac{1}{b}\sum_{i}^b Z^{(i)})$
\STATE $\theta, \phi, \beta, \omega \gets $ Adam $(\nabla_{\theta, \phi, \beta} \mathcal{L}, \{\theta, \phi, \beta, \omega\}, \tau)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{Sampling}
It is not easy to sample $z$ from the learnable prior $p_\lambda(z)$ since the formula of $p_\lambda(z)$ is complicated. Accept/Reject Sampling is also not useful for $p_\lambda$ because Accept/Reject Sampling requires that $p_\lambda(z) / p_\mathcal{N}(z)$ is bounded by a constant $M$ (It means $\beta$ is limited to a very small value), such that a sample could be sampled in $M$ times. 

We notice that Langevin dynamics may be a useful sampling method because it only requires that $\nabla_z \log p_\lambda(z)$ is computable and the initial $z_0$ has an enough high density. Moreover, [] have implemented a Metropolis-Adjusted Langevin Algorithm where the formula of density is similar to $p_\lambda$ and also contains a discriminator term. But the selection of initial $z$ whose density is high enough is still a problem. 

VAEPP indeed combine the theory of VAE and GAN, hence, it could be seen as a VAE-GAN model, therefore, following this philosophy, it is natural to use GAN for helping the sampling of VAEPP. We try to use GAN to learn the distribution $q_\phi(z)$, which has high enough density in $p_\lambda(z)$. The sampling of VAEPP consists of 3 part: generate initial $z_0$ by a GAN, then generate $z \sim p_\lambda(z)$ by Langevin dynamics, and finally generate $x$ by decoder. 


 
 