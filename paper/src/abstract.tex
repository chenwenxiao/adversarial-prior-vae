
\begin{abstract}
 Many approaches to train generative models by distinct training objectives are proposed in the past. Variational auto-encoder (VAE) is an outstanding model of them based on log-likelihood. Some researches show that the simplistic standard Gaussian prior leads to very poor hidden representations in VAE. In this paper, we propose a novel prior Pull-back Prior by Double Metrics Analysis (DMA) for VAE. It involves the discriminator from theory of GAN to enrich the representation ability of prior. Based on it, we propose a more general framework, VAE with Pull-back Prior (VAEPP), which uses the existing techniques of VAE and WGAN to improve the representation ability, sampling quality and stability of training. VAEPP reaches wonderful NLL and comparable FID on MNIST, Static-MNIST, Fashion-MNIST, Omniglot, CIFAR-10 and CelebA. 
\end{abstract}
