
\begin{abstract}
 Many approaches to training generative models by distinct training objectives are proposed in the past. Variational Autoencoder (VAE) is an outstanding model of them based on log-likelihood. In this paper, we propose a novel prior Pull-back Prior by minimizing Wasserstein distance between model distribution and empirical distribution for VAE. It involves the discriminator from theory of GAN to enrich the representation ability of prior. Based on it, we propose a more general framework, VAE with Pull-back Prior (VAEPP), which uses the existing techniques of VAE and WGAN to improve the representation ability, sampling quality and stability of training. VAEPP reaches outstanding NLL and comparable FID on MNIST, Static-MNIST, Fashion-MNIST, Omniglot, CIFAR-10 and CelebA. 
\end{abstract}
