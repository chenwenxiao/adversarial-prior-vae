
\begin{abstract}
 Many approaches to training generative models by distinct training objectives are proposed in the past. Variational Autoencoder (VAE) is an outstanding model of them based on log-likelihood. In this paper, we propose a novel prior Pull-back Prior by adjusting the density of prior by a discriminator that can assess the quality of data for VAE. It involves the discriminator from theory of GAN to enrich prior. Based on it, we propose a more general framework, VAE with Pull-back Prior (VAEPP), which uses the existing techniques of VAE and WGAN to improve the log-likelihood, sampling quality and stability of training. VAEPP reaches outstanding NLL and comparable FID on MNIST, Static-MNIST, Fashion-MNIST, Omniglot, CIFAR-10 and CelebA. 
\end{abstract}
