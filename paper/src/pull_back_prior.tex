\section{Pull-back Prior}\label{sec:pull_back_prior}

\subsection{Intuition}
We follow the way of learnable priors and improve the performance of VAE by apply another powerful learnable prior, called by Pull-back Prior. It is the basic idea of Pull-back Prior that the optimal solution of prior, i.e. aggregated prior, is intractable and another feasible prior which is not optimal theoretically, could lead to better performance than an approximation aggregated prior.

The formula of Pull-back Prior is given by:
\begin{equation}\label{eq:pull_back_prior}
	\ln p_\lambda(z) = \ln p_\mathcal{N}(z) - \beta * D(G(z)) - \ln Z \tag{4}
\end{equation}
where $p_\mathcal{N}(z)$ is a simple prior (\EG standard normal) $\beta$ is a scalar called pull-back weight, $D$ is a discriminator defined on $\mathcal{X}$ (data space), $G$ is an generator defined by $G(z) = \E_{p_\theta(x|z)} x$ and $Z$ is the partition function $Z = \int_{\mathcal{Z}} p_\mathcal{N}(z) \exp\{- \beta * D(G(z))\} \dd z$ ($\mathcal{Z}$ denotes latent space).

A simplistic explanation of Pull-back Prior is given following: We would like to get a more powerful prior than simple prior $p_\mathcal{N}$. A simple way is to improve the density of $z$ which generates better data and decrease the density of $z$ which generates worse data. $D$ is a discriminator to assess the quality of $x$. When $D(x)$ is less, $x$ is more similar to real data and of higher quality. We could pull-back the discriminator from data space to latent space, and function $D(G(z))$ represents the quality of the data generated by $z$. To improve and decrease the density at better $z$ and worse $z$, we modify $p_\mathcal{N}(z)$ by $\beta * D(G(z))$ and then normalize it by $Z$, and finally we obtain the Pull-back Prior. 

\subsection{Inference}~\label{subsec:inference}

Before the starting of inference of Pull-back Prior, we need to review the inference of aggregated posterior. We divide the optimization of $\min_{\theta, \phi, \lambda} \mathcal{L}(\theta, \phi, \lambda)$ into 2 part $\min_{\theta, \phi} \min_{\lambda} \mathcal{L}(\theta, \phi, \lambda)$. Considering the 2nd optimization $\min_\lambda \mathcal{L}(\theta, \phi, \lambda)$, the optimal solution is $p_\lambda(z) = q_\phi(z)$. The key idea of the inference of Pull-back Prior is to set another objective function $\hat{\mathcal{L}}$ for 2nd optimization. Noticing that the ELBO is derived by KL-divergence between $p^*$ and $p_\theta$, a candidate objective function is another divergence. This operation is called Double Metrics Analysis (DMA). 

Choosing another divergence will lead to a new learnable prior rather than $q_\phi$. We could make this new learnable prior feasible and efficient, but however, it will never be the theoretical optimal prior, \IE the essence of DMA is to get an acceptable trade-off between theory and practice. 

We choose Wasserstein distance for 2nd optimization, because it shows wonderful performance in WGAN and has stable theoretical basis in transition theory. Considering following optimization:
\begin{align*}\label{eq:direct_2nd_optimization}
	& \min_{\lambda} \hat{\mathcal{L}}(\theta, \phi, \lambda) = \min_{\lambda} W^1(p_\theta, p^*) = \\
	& \min_{\lambda} \sup_{Lip(D) \leq 1} \{\E_{p_\lambda(z)} \E_{p_\theta(x|z)} D(x)  - \E_{p^*(x)} D(x)\} 
	\tag{5}
\end{align*}

It is hard to get an analytical solution of $\lambda$ directly from \cref{eq:direct_2nd_optimization}, therefore we add an assumption to simplify it. Since $p_\theta(x|z)$ is usually a distribution with small variance, it is rational to assume $\E_{p_\theta(x|z)} D(x) = D(\E_{p_\theta(x|z)} x) = D(G(z))$. Even though, the optimization $\sup_{Lip(D) \leq 1}$ is still tough because we need find optimal $D$ for each $\lambda$. If we restrict $p_\lambda$ near the $p_\mathcal{N}$, this optimization may be approximated by a fixed $D$ obtained in $W^1(p^\dag, p^*)$, where $p^\dag(x) = \E_{p_\mathcal{N}(z)}p_\theta(x|z)$ (distribution generated by $p_\mathcal{N}$). Consequently, the simplified optimization is following:
\begin{align*}\label{eq:final_optimization}
	& \min_{\lambda} \{\E_{p_\lambda(z)} D(G(z))  - \E_{p^*(x)} D(x)\} \tag{6} \\
	{\textbf{s.t. }} & KL(p_\lambda, p_\mathcal{N}) \leq \alpha, \qquad \int_{\mathcal{Z}} p_\lambda(z) \dd z = 1, \\
	& D = \arg \sup_{Lip(D) \leq 1} \{\E_{p_\mathcal{N}(z)} D(G(z))  - \E_{p^*(x)} D(x)\}
\end{align*}

We could solve the simplified optimization \cref{eq:final_optimization} by Lagrange multiplier method introduced by calculus of variation~\cite{gelfand2000calculus}. The Lagrange function with Lagrange multiplier $\eta, \gamma$ is following:
\begin{align*}\label{eq:lagrange_function}
& F(p_\lambda, \eta, \gamma) = \E_{p_\lambda(z)} D(G(z))  - \E_{p^*(x)} D(x) + \\
& \eta (\int_{\mathcal{Z}} p_\lambda(z) \dd z - 1) + \gamma(KL(p_\lambda, p_\mathcal{N}) - \alpha) \tag{7}
\end{align*}

We solve \cref{eq:lagrange_function} by Euler-Lagrange equation:
\begin{equation*}\label{eq:euler_lagrange_eqaution}
	D(G(z)) + \eta - \gamma (\ln p_\lambda(z) + 1 - \ln p_\mathcal{N}(z)) = 0 \tag{8}
\end{equation*}
Therefore, $\ln p_\lambda(z) = \frac{1}{\gamma} D(G(z)) + \ln p_\mathcal{N}(z) + (\frac{\eta}{\gamma} - 1)$ is the optimal solution, which could be organized into \cref{eq:pull_back_prior}. From this inference, we could explain the meaning of $\beta = \frac{1}{\gamma}$ and $Z = \frac{\eta}{\gamma} - 1$. $\beta$ represents how far $p_\lambda$ is from $p_\mathcal{N}$, since $\gamma$ is the Lagrange multiplier of constraint $KL(p_\lambda, p_\mathcal{N}) \leq \alpha$. $Z$ is the partition function since $\eta$ is the Lagrange multiplier of constraint $\int_{\mathcal{Z}} p_\lambda(z) \dd z = 1$.

We obtain the basic formula of Pull-back Prior. However, it remains some troubles about how to optimize it and calculate partition function $Z$ in VAE architecture. 



