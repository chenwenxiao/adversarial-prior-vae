
\section{Pull-back Prior}\label{sec:pull_back_prior}

\subsection{Intuition of Pull-back Prior}\label{subsec:intuition}

The formula of Pull-back Prior is given by:
\begin{equation}\label{eq:pull_back_prior}
	p_\lambda(z) = \frac{1}{Z} p_\mathcal{N}(z) \cdot e^{- \beta D(G(z))} \tag{4}
\end{equation}
where $p_\mathcal{N}$ is a simple prior, $D$ is a discriminator, $G$ is a generator, $Z$ is the partition function $Z = \int_{\mathcal{Z}} p_\mathcal{N}(z) e^{- \beta * D(G(z))} \dd z$ and $\beta$ is a learnable scalar.

A design proposition of Pull-back Prior is that we increase $p_\lambda(z)$ where $z$ generates better data and decrease $p_\lambda(z)$ where $z$ generates worse data. In Pull-back Prior, 
$D$ is a discriminator to assess the quality of $x$, where smaller $D(x)$ indicates $x$ being more similar to real data, as shown in \cref{fig:interpolate}. Such discriminator $D(x)$ is defined on $x$, and the pull-back discriminator on $z$ is defined by $D(G(z))$, representing the ability of $z$ that could generate data with high quality. To improve the density at the better $z$ and decrease the density at the worse $z$, we modify $p_\mathcal{N}(z)$ by $\beta D(G(z))$ and then normalize it by $Z$. 

We obtain the basic formula of Pull-back Prior. The theoretical derivation for Pull-back Prior is provided in \cref{subsec:inference}. However, it remains questions about how to obtain $D$ and $G$, determine $\beta$, and calculate $Z$. 

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.9\columnwidth]{../figures/interpolate}
	\caption{
	The discriminators on above images (generated by linear interpolation of two sample from $q_\phi(z)$), are better at both sides and worse at the middle, which validates the intuition that a discriminator can assess the quality of images. Moreover, the density of $z$ which generates better images will increase, and the density of $z$ which generates worse images will decrease. 
%2825-plot_samples mean and std is 9.528099, 0.0050279954
%[array([[9.547822]], dtype=float32), array([[9.583102]], dtype=float32), array([[9.603901]], dtype=float32), array([[9.626318]], dtype=float32), array([[9.633714]], dtype=float32), array([[9.632847]], dtype=float32), array([[9.631333]], dtype=float32), array([[9.640044]], dtype=float32), array([[9.631817]], dtype=float32), array([[9.654361]], dtype=float32), array([[9.641564]], dtype=float32), array([[9.600273]], dtype=float32), array([[9.559095]], dtype=float32), array([[9.533435]], dtype=float32), array([[9.468535]], dtype=float32)]
%Normalized is [  3.92263684,  10.93934971,  15.07598834,  19.53442519,
%        21.00538915,  20.83295462,  20.53184058,  22.26434018,
%        20.62810161,  25.11179704,  22.56664754,  14.35442841,
%         6.16468344,   1.06125793, -11.84647066]
	}
	\label{fig:interpolate}
\end{figure}

\subsection{How to obtain $D$ and $G$}\label{subsec:determine_D_and_G}
We choose $G(z) = \E_{p_\theta(x|z)} x$ in our model, \IE the mean of the $p_\theta(x|z)$. Notice that $p_\theta(x|z)$ is chosen to be a Gaussian or a Bernouli in our experiments. $G(z)$ is generated by a neural network and it is set as the mean of $p_\theta(x|z)$. 

$D$ plays an important role in Pull-back Prior. We shall propose two ways to obtain $D$ in \cref{subsec:naive_vaepp} and \cref{subsec:improve_of_vaepp}, and compare them later in our experiments. 

\subsection{How to determine $\beta$}\label{subsec:determine_beta}

Theoretically, $\beta$ in \cref{eq:pull_back_prior} represents how far $p_\lambda$ is from $p_\mathcal{N}$, as proved in \cref{subsec:inference}.
%but how to decide the value of $\beta$? When $\beta$ is smaller, the difference between $p_\lambda$ and $p_\mathcal{N}$ is less, \IE the influence of discriminator is severely limited. When $\beta$ is larger, $p_\lambda$ is farther from $p_\mathcal{N}$. Noticing that in \cref{eq:final_optimization}, we simplify the optimization of $D$ by an approximated $D$, if $p_\lambda$ is too far from $p_\mathcal{N}$, this approximation will become invalid. 
%Consequently, $\beta$ should be set to an appropriate value which can't severely limit the influence of discriminator and could ensure that approximated $D$ is valid. 
%It is important to realize that the Pull-back Prior is serving for better ELBO. 
%Whatever the function family of $p_\lambda$ is limited or approximation $D$ is invalid, the ELBO will suffer. 
%Therefore, it is reasonable to search $\beta$ by the optimization for ELBO ($\lambda$ contains $\beta$ and $\omega$, which is the parameters of $D$):
To maximize ELBO, we can obtain the optimal $\beta$ by:
\begin{equation}
	\beta = \arg \min_{\beta} \mathcal{L}(\theta, \phi, \lambda) = \arg \min_{\beta} \mathcal{L}(\theta, \phi, \beta, \omega) \tag{5}
\end{equation}
The gradient $\partial \mathcal{L}/\partial \beta$ is:
\begin{align*}\label{eq:behavior_of_beta}
\frac{\partial \ln Z}{\partial \beta} &= \frac{1}{Z} \int p_\mathcal{N}(z) e^{-\beta D(G(z))} \cdot (-D(G(z))) \dd z \\
&=  \E_{p_\lambda(z)} [- D(G(z))]  \\
\frac{\partial \mathcal{L}}{\partial \beta} &= \E_{q_\phi(z)}[ -D(G(z))] - \frac{\partial \ln Z}{\partial \beta} \\
&= - \E_{q_\phi(z)}[ D(G(z))] + \E_{p_\lambda(z)}[ D(G(z))]   \tag{6}
\end{align*}
The 1st term in \cref{eq:behavior_of_beta} is the mean of the discriminator on reconstructed data (reconstructed data are nearly same as real data in VAE, after only few epochs in training). 
The 2nd term in \cref{eq:behavior_of_beta} is the mean of the discriminator on data generated from $p_\lambda$. 
Hence, $\partial \mathcal{L}/\partial \beta = 0$ means that the discriminator can't distinguish reconstructed data and generated data when the training converges. It coincides with the philosophy of GANs that the discriminator can't distinguish the real data and generated data when the generator is well-trained.	

Noticing that $p_\mathcal{N}$ is a special case of $p_\lambda$ where $\beta = 0$, we shall compare them in experiments.  

\subsection{The lower bound of $Z$}\label{subsec:determine_z}

It is difficult to calculate the exact partition function $Z$. Fortunately in VAE domain, it is acceptable to obtain an upper-bound of $Z$, denoted by $\hat{Z}$, which will not over-estimate log-likelihood and ELBO:
\begin{align*}\label{eq:Z_estimator}
    \hat{Z} = \E_{q_\phi(z)} \frac{f_\lambda(z)}{\hat{q}_\phi(z)} \geq
	\E_{q_\phi(z)}\frac{f_\lambda(z)}{q_\phi(z)} = Z \tag{8}
\end{align*}
where $f_\lambda(z) = p_\mathcal{N}(z) e^{- \beta D(G(z))}$ and $\hat{q}_\phi(z)$ is a lower-bound of $q_\phi(z)$. If we replace $Z$ by $\hat{Z}$ in evaluation and training, we will obtain lower-bounds of log-likelihood and ELBO:
\begin{align*}
	\hat{p}_\theta(x) = \int \frac{p_\theta(x|z) f_\lambda(z)}{\hat{Z}}  \dd z &\leq \int \frac{p_\theta(x|z) f_\lambda(z)}{Z} \dd z = p_\theta(x)  \\
	\mathcal{\hat{K}} = \E_{q_\phi(z)} \frac{1}{\hat{Z}}\ln f_\lambda(z) &\leq \E_{q_\phi(z)} \frac{1}{Z} \ln f_\lambda(z) = \mathcal{K}   \\
    \mathcal{\hat{L}} =  \mathcal{I} + \mathcal{J} + \mathcal{\hat{K}} &\leq \mathcal{I} + \mathcal{J} + \mathcal{K} = \mathcal{L}
\end{align*}
Therefore, $\hat{Z}$ could be used in training and evaluation, which will not over-estimate log-likelihood and ELBO. 

The key of \cref{eq:Z_estimator} is the choice of $\hat{q}_\phi(z)$. It is intractable to compute the exact density of $q_\phi(z)$, but feasible to compute $q_\phi(z|x)$. Based on this idea, we introduce a $\hat{q}_\phi(z)$ which only uses a $q_\phi(z|x^{(j)})$:
\begin{equation*}
	q_\phi(z) = \E_{p^*(x)} q_\phi(z|x) \approx \frac{1}{N}\sum_{i=1}^N q_\phi(z|x^{(i)}) \geq \frac{1}{N} q_\phi(z|x^{(j)})
\end{equation*}
where RHS is $\hat{q}_\phi(z)$, $x^{(j)}$ is the a data and $N$ is the size of training set. $x^{(j)}$ plays an important role here. Considering the sampling process of $q_\phi(z)$, where $z$ is sampled from a real data $x^{(k)}$, $x^{(j)}$ is chosen as the $x^{(k)}$. 
%To reduce the gap between $\hat{q}_\phi(z)$ and $q_\phi(z)$, $q_\phi(z|x^{(j)})$ should be one of the largest in the summation. Therefore, we firstly sample $x^{(j)}$ form dataset, and then sample $z$ from $q_\phi(z|x^{(j)})$. By this way, $q_\phi(z|x^{(j)})$ is large enough. 

In MNIST and other Bernouli image datasets, the number of Bernouli images might be numerous and $\hat{q}_\phi(z)$ might be underestimiated due to the large $N$. 
Since the size of real images is much less than the size of Bernouli images, we use $q_\phi(z|e)$ instead of $q_\phi(z|x)$ to estimate $\hat{q}_\phi(z)$, where $e$ means the real image and $x$ means the Bernouli image. If we have $q_\phi(z|e)$, we could easily obtain $\hat{q}_\phi(z)$ by $\frac{1}{N} q_\phi(z|e^{(j)})$. 

We propose ELBO to train a VAE based on $q_\phi(z|e)$:
\begin{align*}~\label{eq:another_elbo}
	&\E_{p^*(x)} \ln p_\theta(x) \geq \E_{p^*(e)} \E_{p^*(x|e)} \ln \E_{q_\phi(z|e)} \frac{p_\theta(x|z)p_\theta(z)}{q_\phi(z|e)} \\
	 &= \E_{p^*(e)} \E_{p^*(x|e)} \E_{q_\phi(z|e)} \ln \frac{p_\theta(x|z)p_\theta(z)}{q_\phi(z|e)} \tag{7} \\
	 &= \E_{p^*(x)} \ln p^*(x) - \E_{p^*(e)} \E_{p^*(x|e)} KL(q_\phi(z|e), p_\theta(z|x))
\end{align*} 
where $p^*(e)$ denotes the distribution of real images and $p^*(x|e)$ means the Bernouli sampling process from $e$. 
\cref{eq:another_elbo} is similar to the original ELBO~\cref{eq:ELBO}, and the above conclusions about learnable prior holds for \cref{eq:another_elbo} by repeating above derivations. Moreover, the situation without Bernouli images is a special case where $p^*(x|e) = \delta(x - e)$. % Additionally, when $p_\theta(x|z)$ and $p^*(x|e)$ is both Bernouli, term $\E_{p^*(x|e)} \ln p_\theta(x|z)$ has analytical solution $x\log e+(1-x)\log(1-e)$, which could make the training and evaluation more stable (don't need sample $x$). 

Review the estimation of $Z$. By the theory of importance sampling, $p_\lambda$ is the optimal choice for the proposal distribution in estimation of $Z$. However, it is expensive to sample from $p_\lambda$. 
\cite{bauer2019resampled} uses $p_\mathcal{N}$ as the proposal distribution to estimate $Z$ but when $KL(p_\mathcal{N}, p_\lambda)$ is high, the variance of this estimation will be large. 

After training, $KL(q_\phi, p_\lambda)$ is much less than $KL(p_\mathcal{N}, p_\lambda)$. Therefore, we choose $q_\phi(z)$ as the proposal distribution and use a feasible $\hat{q}_\phi(z)$ to replace $q_\phi(z)$ to obtain the lower-bound of $\hat{Z}$ in \cref{eq:Z_estimator}. The variance of $\hat{Z}$ is acceptable in experiments.  

At the begin of training, $\beta$ is small and then $KL(p_\mathcal{N}, p_\lambda)$ is small. Therefore, $p_\mathcal{N}(z)$ could be used together with $q_\phi(z)$, as the proposal distributions in training.
 