\section{Introduction}

How to learn deep generative model that are able to capture complex data distribution in high dimension space, \IE images dataset, is one of the major challenges in machine learning. There are many different approaches to train generative model by distinct training objective. Generative Adversarial Networks (GAN)~\cite{goodfellow2014generative} are famous models based on adversarial training and flow-based models~\cite{dinh2016density,kingma2018glow}, PixelCNN~\cite{van2016conditional}, and variational auto-encoders (VAE)~\cite{kingma2014auto,rezende_stochastic_2014} are outstanding based on log-likelihood. 

VAE uses the variational inference and re-parameterization trick to optimize the evidence lower bound objective of log-likelihood (ELBO). In the past, numerous researches try to enrich the representation ability of true posterior and approximate posterior~\cite{kingma2016improved,tomczak2016improving}, but recently some researches show that a simplistic standard Gaussian prior could lead to poor representation in latent space and it is harmful to the performance of VAE~\cite{tomczak2018vae}. To enrich the representation ability of prior distribution, several learnable prior are proposed~\cite{tomczak2018vae,bauer2019resampled,takahashi2019variational}. Most of them focus on the aggregated posterior which is obtain by the integral of approximate posterior and is shown as the optimal prior. \textbf{We argue that the aggregated posterior is intractable in practice, and it is advisable to choose another feasible learnable prior, which is not optimal theoretically. } 

We introduce Pull-back Prior, to improve the representation ability of prior, through the theory of Wasserstein distance~\cite{arjovsky2017wasserstein} and learnable prior. 
The intuitive interpretation to pull-back prior is following. Firstly, we train a discriminator to assess the quality of images. Then, we pull this discriminator back to latent space by true posterior. Finally we increase the density where pull-back discriminator is good and decrease the density where pull-back discriminator is bad in learnable prior. 
The inference of pull-back prior is similar to the proof that shows aggregated posterior as optimal prior. 
The key difference is that we search the prior that minimizes the Wasserstein distance between empirical distribution and model distribution instead of ELBO (if so, we will obtain aggregated posterior). In our mind, the prior minimizing this Wasserstein distance will also be a powerful representative prior and it could also minimize the KL-distance between empirical distribution and model distribution. 

We design an algorithm to train a VAE with pull-back prior, called VAEPP. Since the training objectives of approximate posterior, true posterior and learnable prior are a little bit different, the performance of VAEPP is limited and the training process is unstable. To achieve more stable training and better log-likelihood, we propose an improvement training method based on SGVB~\cite{kingma2014auto} and gradient penalty term. This method mixes the theory of Wasserstein distance into VAE and extends to more general VAE framework. 

Thanks to the powerful regularizer term of WGAN-GP~\cite{gulrajani2017improved} and WGAN-div~\cite{wu2018wasserstein}, and the practical implement of Langevin dynamics in MEG~\cite{kumar2019maximum}, we enjoy the stable efficient training and sampling process. In these years, out-of-distribution (OoD) is noticed by some researchers, and \cite{nalisnick2018deep} has shown that the model log-likelihood from flow-based models, VAEs, and PixelCNN assign higher density to the images of SVHN~\cite{netzer2011reading} when these models are trained on CIFAR-10 and such peculiar behavior of these models also act on other dataset. It challenges the assumption of generative models that the density of in-distribution is higher and the density of out-of-distribution is lower. In this paper, we will show that the discriminator and the norm of its gradient are as our expectation in WGAN and VAEPP. It reminds us to use these two indicator to help VAE to outcome the OoD problem, and it achieves a powerful performance in OoD testing.

The main contributions of this paper are the following:
\begin{itemize}
	\item We propose Pull-back Prior, which has powerful presentation ability and rises a novel direction to construct new learnable prior. 
	\item We propose VAEPP framework to use the existing techniques of VAE and WGAN to improve the representation ability, sampling quality and stability of training. It is a general and easy-extend VAE framework and may guide a series of models cross the VAE and GAN. 
	\item In log-likelihood metrics, VAEPP outperforms the models without autoregressive components and is competitive to the autoregressive models in log-likelihood metric on vast common datasets. In FID and IS metrics, it outperforms other VAEs and is competitive to GANs in default setting on vast common datasets. 
	\item The combination indicator of VAEPP which involves the log-likelihood and discriminator outperforms other existing OoD detector in OoD testing.
\end{itemize}
