\section{Introduction}

How to learn deep generative model that are able to capture complex data distribution in high dimension space, \EG image datasets, is one of the major challenges in machine learning. There are many different approaches to train generative model by distinct training objective. Generative Adversarial Networks (GAN)~\cite{goodfellow2014generative} are famous models based on adversarial training and flow-based models~\cite{dinh2016density,kingma2018glow}, PixelCNN~\cite{van2016conditional}, and variational auto-encoders (VAE)~\cite{kingma2014auto,rezende_stochastic_2014} are outstanding based on log-likelihood. 

VAE uses the variational inference and re-parameterization trick to optimize the evidence lower bound objective of log-likelihood (ELBO). In the past, numerous researches try to enrich the representation ability of true posterior and variational posterior~\cite{kingma2016improved,tomczak2016improving}, but recently some researches show that a simplistic standard Gaussian prior could lead to poor representation in latent space and it is harmful to the performance of VAE~\cite{tomczak2018vae}. To enrich the representation ability of prior, several learnable prior are proposed~\cite{tomczak2018vae,bauer2019resampled,takahashi2019variational}. Most of them focus on the aggregated posterior which is the integral of variational posterior and is shown as the optimal prior. \textbf{We argue that the aggregated posterior is intractable in practice, and it is advisable to choose another feasible learnable prior, which minimizes Wasserstein distance. } 

We introduce Pull-back Prior, to improve the representation ability of prior, through the theory of Wasserstein distance~\cite{arjovsky2017wasserstein} and learnable prior. The key idea of inference is to search the analytical optimal prior which minimizes Wasserstein distance between model distribution and empirical distribution by calculus of variations. 
The intuitive interpretation of Pull-back Prior is easy-understanding: Firstly, a discriminator is trained for assessing the quality of images. Then, this discriminator is pull-back to latent space by true posterior. Finally, we increase the density where pull-back discriminator is good and decrease the density where pull-back discriminator is bad in learnable prior. 
%The inference of pull-back prior is similar to the proof that shows aggregated posterior as optimal prior. 
%The key difference is that we search the prior that minimizes the Wasserstein distance between empirical distribution and model distribution instead of ELBO (if so, we will obtain aggregated posterior). 

We design a simple algorithm to train VAE with Pull-back Prior, called Naive VAEPP. Since training objectives of variational posterior, true posterior and learnable prior are different, the performance of VAEPP is limited and the training process is unstable. To achieve better log-likelihood and more stable training, we propose VAEPP, based on SGVB~\cite{kingma2014auto} and gradient penalty term, which mixes Wasserstein distance into VAE and extends to a more general VAE framework. 

Thanks to the powerful gradient pelnaty term of WGAN-GP~\cite{gulrajani2017improved} and WGAN-div~\cite{wu2018wasserstein}, and the practical implement of Langevin dynamics in MEG~\cite{kumar2019maximum}, we enjoy the stable efficient training and sampling process. 
% In these years, out-of-distribution (OoD) is noticed by some researchers, and \cite{nalisnick2018deep} has shown that the model log-likelihood from flow-based models, VAEs, and PixelCNN assign higher density to the images of SVHN~\cite{netzer2011reading} when these models are trained on CIFAR-10 and such peculiar behavior of these models also act on other dataset. It challenges the assumption of generative models that the density of in-distribution is higher and the density of out-of-distribution is lower. In this paper, we will show that the discriminator and the norm of its gradient are as our expectation in VAEPP. It reminds us to use these indicators to help VAE to outcome the OoD problem, and it achieves a powerful performance in OoD testing.

The main contributions of this paper are the following:
\begin{itemize}
	\item We propose Pull-back Prior, which has powerful presentation ability and rises a novel direction to construct new learnable prior. 
	\item We propose VAEPP framework to use the existing techniques of VAE and WGAN to improve the representation ability, sampling quality and stability of training. It is a general and easy-extend VAE framework and may guide a series of models cross the VAE and GAN. 
	\item In log-likelihood metrics, VAEPP outperforms the models without autoregressive components and is competitive to the autoregressive models in log-likelihood metric on vast common datasets. In FID and IS metrics, it outperforms other VAEs and is competitive to GANs in default setting on vast common datasets. 
	% \item The combination indicator of VAEPP which involves the log-likelihood and discriminator outperforms other existing OoD detector in OoD testing.
\end{itemize}
