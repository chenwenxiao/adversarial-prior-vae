\section{Introduction}

How to learn deep generative model that are able to capture complex data distribution in high dimension space, \IE images dataset, is one of the major challenges in machine learning. There are many different approaches to train generative model by distinct training objective, which are divided into two categories: implicit and explicit, by whether it can calculate log-likelihood. Generative Adversarial Networks are famous models of explicit models and flow-based models, PixelCNN, and variational auto-encoders (VAE) are outstanding in implicit models. 

VAE uses the variational inference and reparameterization trick to optimize the evidence lower bound objective of log-likelihood (ELBO). In the past, numerous researches try to enrich the representation ability of posterior and approximate posterior, but recently some researches show that a simple standard gaussian prior could lead to poor representation in latent space and is harmful to the performance of VAE. To enrich the representation ability of prior distribution, several learnable prior are proposed. They all focus on the aggregated posterior which is obtain by the integral of approximate posterior and is shown as the optimal prior. \textbf{We argue that the aggregated posterior is intractable in practice, and it is advisable to choose a feasible learnable prior, which is not optimal theoretically. } 

We introduce pull-back prior, which involves the theory of Wasserstein distance and true posterior. The intuitive interpretation to pull-back prior is following. Firstly, we train a discriminator to assess the quality of images. Then, we pull this discriminator back to latent space by true posterior. Finally we increase the density where pull-back discriminator is good and decrease the density where pull-back discriminator is bad in learnable prior. The inference of pull-back prior is similar to the proof that shows aggregated posterior as optimal prior. The key difference is that we search the prior that minimizes the Wasserstein distance between empirical distribution and model distribution instead of ELBO (if so, we will obtain aggregated posterior). In our mind, the prior minimizing this Wasserstein distance will also be a powerful representative prior and it could also minimize the KL-distance between empirical distribution and model distribution which is final goal of VAE. VAE with pull-back prior is called VAEPP. 

It means that the training objectives of approximate posterior, posterior and learnable prior are a little bit different. To achieve more stable training and better log-likelihood, we propose an improvement training method which are totally based SGVB. Based on it, we could review the pull-back prior from traditional viewpoint, and relate it to the aggregated posterior. This method mixes the theory of Wasserstein distance into VAE and extends to more general VAE framework. 

Thanks to the powerful regularizer term of WGAN-GP and WGAN-div, and the practical implement of Langevin dynamics in MEG, we enjoy the stable efficient training and sampling process. About the benefit of mixing the GAN and VAE, it is not limited to the training and sampling. In these years, out-of-distribution (OoD) is noticed by some researchers, and one of them has shown that the model log-likelihood from flow-based models, VAEs, and PixelCNN cannot distinguish the images from CIFAR-10 and SVHN. Moreover, these models even assign higher density to the images of SVHN when these models are trained on CIFAR-10. It challenges the assumption of generative models that the density of in-distribution is higher and the density of out-of-distribution is lower. In this paper, we will show that the discriminator and the norm of its gradient are as our expectation in WGAN. We hope that these two indicator could also help VAE to outcome the OoD problem especially on CIFAR-10 and SVHN. Our experiments show a negative result that VAEPP also assigns higher density to SVHN and a positive result that the discriminator and the norm of its gradient holds right in VAEPP. It reminds us to create an overall indicator consisting of these 3 indicators (log-likelihood, discriminator and norm of its gradient). The overall achieve a powerful performance in OoD testing.  

The main contributions of this paper are the following:
\begin{itemize}
	\item We propose a new prior that involves the discriminator and true posterior. It follows the researches of learnable prior and rise a new direction that using true posterior instead of approximate posterior and theory of GAN. 
	\item We propose VAEPP framework, and implement it on vast common datasets. It outperforms the models without autoregressive components and is comparable to the autoregressive models. 
	\item We obtain a general framework which involves VAE and GAN. In this framework, the existing techniques of VAE and GAN could be used to improve the representation ability, sampling quality and stability in training. It may guide a series of models cross the VAE and GAN. 
	\item We find that the discriminator and the norm of its gradient are good indicators in OoD and combine log-likelihood, discriminator and the norm of its gradient in to an overall indicator to detect OoD. It reaches outstanding performance in OoD testing.
\end{itemize}
